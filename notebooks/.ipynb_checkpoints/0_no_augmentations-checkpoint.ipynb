{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faef02ae-2660-4ea5-b1f9-00d49dee280d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.41.1)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.8.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.19.1)\n",
      "Requirement already satisfied: rouge_score in /usr/local/lib/python3.9/dist-packages (0.1.2)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.9/dist-packages (0.30.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.23.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2024.3.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.9/dist-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.5.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/lib/python3/dist-packages (from rouge_score) (1.14.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (1.12.1+cu116)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (18.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers nltk datasets rouge_score accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5b91dc6-7aba-405e-af56-ef8db3916464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "import numpy as np\n",
    "import nltk\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, load_metric\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44eda635-3fcf-40aa-8021-204437fc3783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Relation</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Component-Whole(e2,e1)</td>\n",
       "      <td>system as described above has its greatest app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Other</td>\n",
       "      <td>E1_START child E1_END was carefully wrapped an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Instrument-Agency(e2,e1)</td>\n",
       "      <td>E1_START author E1_END of a keygen uses a E2_S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Other</td>\n",
       "      <td>misty E1_START ridge E1_END uprises from the E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Member-Collection(e1,e2)</td>\n",
       "      <td>E1_START student E1_END E2_START association E...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Relation                                           Sentence\n",
       "0    Component-Whole(e2,e1)  system as described above has its greatest app...\n",
       "1                     Other  E1_START child E1_END was carefully wrapped an...\n",
       "2  Instrument-Agency(e2,e1)  E1_START author E1_END of a keygen uses a E2_S...\n",
       "3                     Other  misty E1_START ridge E1_END uprises from the E...\n",
       "4  Member-Collection(e1,e2)  E1_START student E1_END E2_START association E..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_data_frame(file_path):\n",
    "\n",
    "  with open(file_path, \"r\") as file:\n",
    "      data = file.readlines()\n",
    "\n",
    "  relations = []\n",
    "  sentences = []\n",
    "\n",
    "  for line in data:\n",
    "    parts = line.strip().split(\" \")\n",
    "    relation = parts[1]\n",
    "    sentence = \" \".join(parts[3:])\n",
    "    relations.append(relation)\n",
    "    sentences.append(sentence)\n",
    "\n",
    "  return pd.DataFrame({\"Relation\": relations, \"Sentence\": sentences})\n",
    "\n",
    "train_data = create_data_frame(\"./data/train_attn_sp.txt\")\n",
    "test_data = create_data_frame(\"./data/test_attn_sp.txt\")\n",
    "validation_data = create_data_frame(\"./data/val_attn_sp.txt\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "104aa9eb-fee9-462a-84d4-9f7b448b8e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_train_data = Dataset.from_pandas(train_data)\n",
    "converted_test_data = Dataset.from_pandas(test_data)\n",
    "converted_val_data = Dataset.from_pandas(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d25dd6a7-08cd-4090-9882-19e40dcfe8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_tokenize_preprocess(batch, tokenizer, max_source_length, max_target_length):\n",
    "    source, target = batch[\"Sentence\"], batch[\"Relation\"]\n",
    "    source_tokenized = tokenizer(\n",
    "        source, padding=\"max_length\", truncation=True, max_length=max_source_length\n",
    "    )\n",
    "    target_tokenized = tokenizer(\n",
    "        target, padding=\"max_length\", truncation=True, max_length=max_target_length\n",
    "    )\n",
    "\n",
    "    batch = {k: v for k, v in source_tokenized.items()}\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in l]\n",
    "        for l in target_tokenized[\"input_ids\"]\n",
    "    ]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a5a61f1-50b5-4564-bd85-9e7dfe5540fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-base')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a11530f5-6435-470e-bc2a-0ba8601dc829",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_max_length = 128\n",
    "decoder_max_length = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "422f4e46-d41c-4c43-8040-d919893fa934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0bba1cba1444acbab5ae219d4d8a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_train_data = converted_train_data.map(\n",
    "    lambda batch: batch_tokenize_preprocess(\n",
    "        batch, tokenizer, encoder_max_length, decoder_max_length\n",
    "    ),\n",
    "    batched=True,\n",
    "    remove_columns=converted_train_data.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "719456e7-5d52-44d3-983e-980fbf08da86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec8a80362a3488587d9935c8f2cd30f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/792 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_validation_data = converted_val_data.map(\n",
    "    lambda batch: batch_tokenize_preprocess(\n",
    "        batch, tokenizer, encoder_max_length, decoder_max_length\n",
    "    ),\n",
    "    batched=True,\n",
    "    remove_columns=converted_train_data.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3fa8d04-7964-45c9-9c77-866874c8231b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f114a2282554440b924fc4d8efbbc79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2717 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_test_data = converted_test_data.map(\n",
    "    lambda batch: batch_tokenize_preprocess(\n",
    "        batch, tokenizer, encoder_max_length, decoder_max_length\n",
    "    ),\n",
    "    batched=True,\n",
    "    remove_columns=converted_train_data.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbdb2b79-bdbb-480a-aa75-70ab45725020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3036/3803988065.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = datasets.load_metric(\"rouge\")\n",
      "/usr/local/lib/python3.9/dist-packages/datasets/load.py:759: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\", quiet=True)\n",
    "metric = datasets.load_metric(\"rouge\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    if preds is None or labels is None:\n",
    "        print(\"Warning: Either preds or labels is None.\")\n",
    "        return [], []\n",
    "\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "569fc02b-e931-41af-ab9f-144240144d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "\n",
    "    # Extract a few results from ROUGE\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "765a633e-e0c5-4154-990e-d0e3a220b127",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./0_no_augmentaions\",\n",
    "    num_train_epochs=20,\n",
    "    # save_strategy = 'steps',\n",
    "    save_strategy = 'epoch',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=1e-5,\n",
    "    warmup_ratio=0.2,\n",
    "    weight_decay=1e-3,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=200,\n",
    "    save_total_limit=1,\n",
    "    report_to = 'none',\n",
    ")\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=batch_train_data,\n",
    "    eval_dataset=batch_validation_data,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55dc6d44-1606-4401-99b8-817b50fd9366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18020' max='18020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18020/18020 44:10, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.361400</td>\n",
       "      <td>0.225624</td>\n",
       "      <td>58.570100</td>\n",
       "      <td>34.638000</td>\n",
       "      <td>50.726000</td>\n",
       "      <td>50.726000</td>\n",
       "      <td>13.664100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.176100</td>\n",
       "      <td>0.123649</td>\n",
       "      <td>71.969700</td>\n",
       "      <td>60.606100</td>\n",
       "      <td>69.160400</td>\n",
       "      <td>69.160400</td>\n",
       "      <td>13.233600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.098300</td>\n",
       "      <td>0.074503</td>\n",
       "      <td>79.356100</td>\n",
       "      <td>72.979800</td>\n",
       "      <td>78.409100</td>\n",
       "      <td>78.409100</td>\n",
       "      <td>13.180600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.077900</td>\n",
       "      <td>0.068664</td>\n",
       "      <td>81.060600</td>\n",
       "      <td>75.126300</td>\n",
       "      <td>80.303000</td>\n",
       "      <td>80.239900</td>\n",
       "      <td>13.204500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.064100</td>\n",
       "      <td>0.060616</td>\n",
       "      <td>82.929700</td>\n",
       "      <td>71.633000</td>\n",
       "      <td>82.079700</td>\n",
       "      <td>82.167700</td>\n",
       "      <td>12.455800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.048700</td>\n",
       "      <td>0.067385</td>\n",
       "      <td>84.262300</td>\n",
       "      <td>74.002500</td>\n",
       "      <td>83.520500</td>\n",
       "      <td>83.572300</td>\n",
       "      <td>12.568200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.062774</td>\n",
       "      <td>86.174200</td>\n",
       "      <td>75.412500</td>\n",
       "      <td>85.459500</td>\n",
       "      <td>85.448200</td>\n",
       "      <td>12.540400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.037900</td>\n",
       "      <td>0.063019</td>\n",
       "      <td>85.580500</td>\n",
       "      <td>75.155700</td>\n",
       "      <td>85.089000</td>\n",
       "      <td>85.124300</td>\n",
       "      <td>12.436900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.026100</td>\n",
       "      <td>0.074421</td>\n",
       "      <td>87.005500</td>\n",
       "      <td>76.346800</td>\n",
       "      <td>86.532000</td>\n",
       "      <td>86.558300</td>\n",
       "      <td>12.524000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.073447</td>\n",
       "      <td>86.798000</td>\n",
       "      <td>75.202000</td>\n",
       "      <td>86.269700</td>\n",
       "      <td>86.265900</td>\n",
       "      <td>12.363600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>0.072887</td>\n",
       "      <td>87.319600</td>\n",
       "      <td>75.467200</td>\n",
       "      <td>86.829600</td>\n",
       "      <td>86.748400</td>\n",
       "      <td>12.268900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>0.081158</td>\n",
       "      <td>87.232100</td>\n",
       "      <td>75.663900</td>\n",
       "      <td>86.517000</td>\n",
       "      <td>86.541300</td>\n",
       "      <td>12.401500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.083580</td>\n",
       "      <td>87.387300</td>\n",
       "      <td>75.458800</td>\n",
       "      <td>86.945300</td>\n",
       "      <td>86.895700</td>\n",
       "      <td>12.234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>0.100875</td>\n",
       "      <td>87.148300</td>\n",
       "      <td>76.195300</td>\n",
       "      <td>86.661300</td>\n",
       "      <td>86.652200</td>\n",
       "      <td>12.457100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.104960</td>\n",
       "      <td>86.679300</td>\n",
       "      <td>75.172600</td>\n",
       "      <td>86.212600</td>\n",
       "      <td>86.208100</td>\n",
       "      <td>12.311900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.104798</td>\n",
       "      <td>87.500000</td>\n",
       "      <td>75.820700</td>\n",
       "      <td>87.058100</td>\n",
       "      <td>87.058100</td>\n",
       "      <td>12.310600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.108578</td>\n",
       "      <td>87.466600</td>\n",
       "      <td>76.115300</td>\n",
       "      <td>86.985300</td>\n",
       "      <td>87.002900</td>\n",
       "      <td>12.359800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.106838</td>\n",
       "      <td>87.761500</td>\n",
       "      <td>75.976400</td>\n",
       "      <td>87.288100</td>\n",
       "      <td>87.261000</td>\n",
       "      <td>12.351000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.108559</td>\n",
       "      <td>87.319600</td>\n",
       "      <td>75.484000</td>\n",
       "      <td>86.763600</td>\n",
       "      <td>86.737500</td>\n",
       "      <td>12.327000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.107646</td>\n",
       "      <td>87.892300</td>\n",
       "      <td>76.018500</td>\n",
       "      <td>87.364700</td>\n",
       "      <td>87.405300</td>\n",
       "      <td>12.324500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18020, training_loss=0.12972239907107397, metrics={'train_runtime': 2651.3384, 'train_samples_per_second': 54.373, 'train_steps_per_second': 6.797, 'total_flos': 1.09874509774848e+16, 'train_loss': 0.12972239907107397, 'epoch': 20.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e067a1e-3de4-48c0-8b9e-e6bcbefb6764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1(key, prediction, none_id):\n",
    "    correct_by_relation = ((key == prediction) & (prediction != none_id)).astype(np.int32).sum()\n",
    "    guessed_by_relation = (prediction != none_id).astype(np.int32).sum()\n",
    "    gold_by_relation = (key != none_id).astype(np.int32).sum()\n",
    "\n",
    "    prec_micro = 1.0\n",
    "    if guessed_by_relation > 0:\n",
    "        prec_micro = float(correct_by_relation) / float(guessed_by_relation)\n",
    "    recall_micro = 1.0\n",
    "    if gold_by_relation > 0:\n",
    "        recall_micro = float(correct_by_relation) / float(gold_by_relation)\n",
    "    f1_micro = 0.0\n",
    "    if prec_micro + recall_micro > 0.0:\n",
    "        f1_micro = 2.0 * prec_micro * recall_micro / (prec_micro + recall_micro)\n",
    "    return prec_micro, recall_micro, f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "495b920a-3e03-4aa4-a94a-d3720b5e29f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score():\n",
    "    \n",
    "    pred = trainer.predict(\n",
    "            batch_test_data, metric_key_prefix=\"predict\", max_length=32, num_beams=4\n",
    "        )\n",
    "\n",
    "    label_seq2seq = []\n",
    "    pred_seq2seq = []\n",
    "    \n",
    "    print('start generate pred_seq2seq')\n",
    "\n",
    "    for k, d in tqdm(enumerate(batch_test_data)):\n",
    "        \n",
    "        tt = d['labels']\n",
    "        temp_label = tokenizer.decode(tt[:np.sum(np.array(tt) != -100)], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "        temp_pred = tokenizer.decode(pred[0][k], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "        label_seq2seq.append(temp_label)\n",
    "        pred_seq2seq.append(temp_pred)\n",
    "\n",
    "    print('*****finish predict*****')\n",
    "    def func(x):\n",
    "        if x in label_seq2seq:\n",
    "            return x\n",
    "        else:\n",
    "            return 'Other'\n",
    "        \n",
    "    pred_seq2seq = [func(x) for x in pred_seq2seq]\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    df['label'] = label_seq2seq\n",
    "    df['pred'] = pred_seq2seq\n",
    "    print('*****finish df*****')\n",
    "    \n",
    "    lb = LabelEncoder()\n",
    "    lb.fit(list(df['label']))\n",
    "    label_lb = lb.transform(list(df['label']))\n",
    "    pred_lb = lb.transform(list(df['pred']))\n",
    "\n",
    "    print('*****finish encode*****')\n",
    "\n",
    "    P, R, F1 = get_f1(label_lb, pred_lb, lb.transform(['Other'])[0])\n",
    "    \n",
    "    return P, R, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c929767-4ba9-46ee-9672-f085903027d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start generate pred_seq2seq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2717it [00:00, 4942.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****finish predict*****\n",
      "*****finish df*****\n",
      "*****finish encode*****\n",
      "score:, P:0.855893377759267, R:0.9080866106937693, F1 :0.8812178387650085 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "P, R, F1 = get_score()\n",
    "print('{}:, P:{}, R:{}, F1 :{} \\n'.format('score', P, R, F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "120adb06-bccb-4765-95e5-f5eedc0b1c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e440bb556f425aa030b8f49190d23b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf5d8186-c31d-4e96-96d7-3016b715be80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2264a6b3090042b3918b638b3a72aec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Mazin100/no_augmentation_relation_extractoin/commit/3dda9a74d94ca2f98a98a2e805125a1fe3d3b6ec', commit_message='Upload BartForConditionalGeneration', commit_description='', oid='3dda9a74d94ca2f98a98a2e805125a1fe3d3b6ec', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model_path = \"../models/no_augmentations/checkpoint-18020/\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(trained_model_path)\n",
    "model.push_to_hub(\"Mazin100/no_augmentation_relation_extractoin\", model_path=trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "feae6498-8dff-4f40-8ee9-cddec907b37f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed0e7d5e2a24efab48257f83fed550d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Mazin100/no_augmentation_relation_extractoin/commit/ec7923c6c43b1a5c1055e8b85ffca6afa8649b54', commit_message='Upload tokenizer', commit_description='', oid='ec7923c6c43b1a5c1055e8b85ffca6afa8649b54', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(trained_model_path)\n",
    "tokenizer.push_to_hub(\"Mazin100/no_augmentation_relation_extractoin\", model_path=trained_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
